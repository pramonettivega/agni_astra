{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on FIA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Libraries for modeling\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report \n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data file - FIATreeSpeciesCode_pft.csv\n",
    "df_fia_species = pd.read_csv(\"./data/FIATreeSpeciesCode_pft.csv\", sep=\";\")\n",
    "df_fia_species_imp = df_fia_species[['SPCD', 'COMMON_NAME', 'PFT']].copy()\n",
    "\n",
    "# Reading the data file - CA_TREE.csv\n",
    "df_fia_tree = pd.read_csv(\"./data/CA_TREE.csv\", low_memory=False)\n",
    "df_fia_tree_imp = df_fia_tree[['STATECD', 'PLOT', 'PLT_CN', 'UNITCD', 'COUNTYCD', 'TREE', 'SPCD', 'DIA', 'HT', 'CR']].copy()\n",
    "\n",
    "# Reading the data file - CA_PLOT.csv\n",
    "df_fia_plot = pd.read_csv(\"./data/CA_PLOT.csv\", low_memory=False)\n",
    "df_fia_plot_imp = df_fia_plot[['PLOT_STATUS_CD', 'LAT', 'LON', 'ELEV', 'ECOSUBCD', 'CN']].copy()\n",
    "\n",
    "# Analyzing the shape\n",
    "print(df_fia_species_imp.shape)\n",
    "print(df_fia_tree_imp.shape)\n",
    "print(df_fia_plot_imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the FIA Species and FIA tree based on SPCD\n",
    "df_fia = df_fia_tree_imp.merge(df_fia_species_imp, on=\"SPCD\", how=\"left\")\n",
    "# Merging the resulting FIA dataset and FIA plot based on PLOT_CN and CN\n",
    "df_fia = df_fia.merge(df_fia_plot_imp, left_on=\"PLT_CN\", right_on=\"CN\", how=\"left\")\n",
    "print(df_fia.shape)\n",
    "df_fia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inches to cm\n",
    "df_fia['DIA'] = df_fia['DIA'] * 2.54 \n",
    "# Calculating the basal area\n",
    "df_fia['BasalA'] = (np.pi * df_fia['DIA']**2) / (4 * 144)\n",
    "df_fia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the distribution of PFT\n",
    "df_fia['PFT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe the data is particularly skewed towards Evergreen conifer. However, we wanted to develop a general model particularly focusing on the sites\n",
    "\n",
    "    \"Shaver Lake (SHA)\": \"M261Ep\",\n",
    "\n",
    "    \"Sedgwick Reserve (SDR)\": \"261Ba\",\n",
    "\n",
    "    \"Calaveras Big Trees State Park\": \"M261Em\",\n",
    "\n",
    "    \"Pacific Union College (PUC)\": \"263Am\",\n",
    "\n",
    "    \"Independence Lake (IND)\": \"M261Ej\",\n",
    "\n",
    "    \"Winton-Schaads VMP (WIN)\" :\"M261Em\",\n",
    "\n",
    "So we need to filter these sites based on ECOSUBCD, but first let's select the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the important features\n",
    "df_fia_important_features = df_fia[['DIA', 'HT', 'BasalA', 'LAT', 'LON', 'COMMON_NAME', 'SPCD', 'ECOSUBCD', 'PFT']].copy()\n",
    "df_fia_important_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "print(\"Shape before:\", df_fia_important_features.shape)\n",
    "df_fia_important_features = df_fia_important_features.dropna(axis=0).copy()\n",
    "print(\"Shape before:\", df_fia_important_features.shape)\n",
    "df_fia_important_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortlisting the data based on the sites to focus on\n",
    "\n",
    "# Define the ECOSUBCDs to keep\n",
    "ecosubcd_keep = ['M261Ep', '261Ba', 'M261Em', '263Am', 'M261Ej']\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_fia_filtered = df_fia_important_features[df_fia_important_features['ECOSUBCD'].isin(ecosubcd_keep)]\n",
    "# Shuffle the filtered DataFrame\n",
    "df_fia_filtered_shuffled = df_fia_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Keeping the original ECOSUBCD column for later use\n",
    "df_fia_ecosubcd = df_fia_filtered_shuffled['ECOSUBCD']\n",
    "\n",
    "# One hot encoding the categorical variable ECOSUBCD\n",
    "df_fia_encoded = pd.get_dummies(df_fia_filtered_shuffled, columns=['ECOSUBCD'])\n",
    "df_fia_encoded = df_fia_encoded.drop(['ECOSUBCD_M261Ep'], axis=1)\n",
    "df_fia_encoded['ECOSUBCD'] = df_fia_ecosubcd\n",
    "df_fia_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the corrections to the PFT column based on the email\n",
    "replace_dict = {'Deciduous': 'Deciduous broadleaf'} \n",
    "df_fia_encoded['PFT'] = df_fia_encoded['PFT'].replace(replace_dict)\n",
    "df_encoded = df_fia_encoded[df_fia_encoded['PFT'] != 'Broadleaf']\n",
    "df_encoded['PFT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the distribution of PFT across different ECOSUBCDs\n",
    "df_encoded.groupby('ECOSUBCD')['PFT'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of sites are skewed towards a particular PFT type, so location plays an important role in this distribution. When we consider this to our intuition, it makes sense as certain types of trees are located in the certain areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transform the data for training and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_encoded.copy()\n",
    "\n",
    "# Defining the independent and dependent variables\n",
    "independent_variables = ['DIA', 'HT', 'BasalA', 'LAT', 'LON',\n",
    "                         'ECOSUBCD_261Ba', 'ECOSUBCD_263Am',\n",
    "                         'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em']\n",
    "dependent_variable = \"PFT\"\n",
    "include_variables = independent_variables + [dependent_variable]\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_clean[independent_variables]\n",
    "y = df_clean[dependent_variable]\n",
    "\n",
    "# Printing the class distribution\n",
    "print(\"Class distribution in the dataset:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Encoded classes: {list(le.classes_)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
    "    X, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Original Training set size: {len(X_train)}\")\n",
    "print(f\"Original Test set size: {len(X_test)}\")\n",
    "\n",
    "# Feature Scaling\n",
    "numerical_cols = ['DIA','HT', 'BasalA', 'LAT', 'LON']\n",
    "categorical_cols = [col for col in independent_variables if col not in numerical_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale numerical features\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled_num = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "X_train_processed = np.hstack((X_train_scaled_num, X_train[categorical_cols].values))\n",
    "X_test_processed = np.hstack((X_test_scaled_num, X_test[categorical_cols].values))\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_processed, y_train_encoded, test_size=0.30, random_state=42, stratify=y_train_encoded\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Best Base Models ---\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=317,\n",
    "    max_depth=23,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    bootstrap=True,\n",
    "    criterion='log_loss',\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=317,\n",
    "    max_depth=23,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    bootstrap=True,\n",
    "    criterion='log_loss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=14,\n",
    "    min_samples_split=13,\n",
    "    min_samples_leaf=3,\n",
    "    max_features=None,\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    n_estimators=71,\n",
    "    max_samples=0.616,\n",
    "    max_features=0.987,\n",
    "    bootstrap=False,\n",
    "    bootstrap_features=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Meta-Learner ---\n",
    "meta_learner = XGBClassifier(\n",
    "    n_estimators=180,\n",
    "    max_depth=13,\n",
    "    learning_rate=0.0116,\n",
    "    subsample=0.803,\n",
    "    colsample_bytree=0.886,\n",
    "    gamma=0.994,\n",
    "    reg_alpha=0.711,\n",
    "    reg_lambda=0.790,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Stacking Ensemble ---\n",
    "final_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('et', et),\n",
    "        ('dt', dt),\n",
    "        ('bagging', bagging)\n",
    "    ],\n",
    "    final_estimator=meta_learner,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "final_model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    ")\n",
    "\n",
    "print(f\"Final model training finished. Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Predict and Evaluate on the Test Set ---\n",
    "print(\"Evaluating model on the test set...\")\n",
    "\n",
    "# Predict using the final model\n",
    "predicted_pft_test_encoded = final_model.predict(X_test_processed)\n",
    "\n",
    "# Decode predictions for readability\n",
    "predicted_labels_decoded = le.inverse_transform(predicted_pft_test_encoded)\n",
    "\n",
    "# Get original test labels for comparison\n",
    "original_test_labels = le.inverse_transform(y_test_encoded)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(\n",
    "    original_test_labels,\n",
    "    predicted_labels_decoded,\n",
    "    zero_division=0,\n",
    "    target_names=le.classes_\n",
    ")\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of sites are skewed towards a particular PFT, so location might play an important role in this distribution. When we consider this to our intuition, it makes sense as certain types of trees are located in the certain areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TLS Data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the TLS data\n",
    "tls_treelist_df = pd.read_csv(\"./data/TLS_treelist.csv\", index_col=0)\n",
    "df_tls_plot_identification = pd.read_csv(\"./data/blk_plot_identification.csv\")\n",
    "df_tls_data = tls_treelist_df.merge(df_tls_plot_identification,on=\"plot_blk\", how=\"left\")\n",
    "df_tls_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of the site names\n",
    "df_tls_data['site_name_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before:\",df_tls_data.shape)\n",
    "# Dropping the null values\n",
    "df_tls_data = df_tls_data.dropna(axis=0).copy()\n",
    "print(\"Shape after:\",df_tls_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X, Y coordinates to Latitude and Longitude\n",
    "gdfs = []\n",
    "for crs in df_tls_data[\"plot_coord_srs\"].unique():\n",
    "    df_crs = df_tls_data[df_tls_data[\"plot_coord_srs\"] == crs]\n",
    "    geometry = [Point(xy) for xy in zip(df_crs[\"X\"], df_crs[\"Y\"])]\n",
    "    gdf = gpd.GeoDataFrame(df_crs, geometry=geometry, crs=f\"EPSG:{crs}\")\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    gdfs.append(gdf)\n",
    "\n",
    "# Recombine the GeoDataFrames\n",
    "gdf = pd.concat(gdfs)\n",
    "gdf[\"longitude\"] = gdf.geometry.x\n",
    "gdf[\"latitude\"] = gdf.geometry.y\n",
    "\n",
    "df_tls_data['LAT'] = gdf['latitude']\n",
    "df_tls_data['LON'] = gdf['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will map the site names to the corresponding ECOSUBCDs\n",
    "#  Define mapping\n",
    "site_to_ecosubcd = {\n",
    "    \"Shaver Lake (SHA)\": \"M261Ep\",\n",
    "    \"Sedgwick Reserve (SDR)\": \"261Ba\",\n",
    "    \"Calaveras Big Trees State Park\": \"M261Em\",\n",
    "    \"Pacific Union College (PUC)\": \"263Am\",\n",
    "    \"Independence Lake (IND)\": \"M261Ej\",\n",
    "    \"Winton-Schaads VMP (WIN)\" :\"M261Em\",\n",
    "}\n",
    "\n",
    "# Create Ecosubcd column based on site names\n",
    "df_tls_data['ECOSUBCD'] = df_tls_data['site_name_label'].map(site_to_ecosubcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important features from the TLS data\n",
    "TLS_X = df_tls_data[['DBH', 'H', 'BasalA', 'LAT', 'LON', 'ECOSUBCD']].copy()\n",
    "# Renaming columns for consistency\n",
    "TLS_X.columns = ['DIA', 'HT', 'BasalA', 'LAT', 'LON', 'ECOSUBCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding the categorical variable ECOSUBCD\n",
    "TLS_X_ECOSUBCD = TLS_X['ECOSUBCD'].copy()\n",
    "TLS_X = pd.get_dummies(TLS_X, columns=['ECOSUBCD'])\n",
    "# Dropping one of the dummy variables to avoid the dummy variable trap\n",
    "TLS_X = TLS_X.drop(['ECOSUBCD_M261Ep'], axis=1)\n",
    "TLS_X = TLS_X[['DIA', 'HT', 'BasalA', 'LAT', 'LON',\n",
    "                         'ECOSUBCD_261Ba', 'ECOSUBCD_263Am',\n",
    "                         'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em']]\n",
    "TLS_X['ECOSUBCD'] = TLS_X_ECOSUBCD\n",
    "\n",
    "# Shuffle the filtered DataFrame\n",
    "TLS_X_shuffled = TLS_X.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "TLS_X_shuffled = TLS_X.copy()\n",
    "\n",
    "# dropping the null values\n",
    "TLS_X_shuffled = TLS_X_shuffled.dropna(axis=0).copy()\n",
    "TLS_X_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables\n",
    "\n",
    "independent_variables = ['DIA','HT', 'BasalA', 'LAT', 'LON',\n",
    "                         'ECOSUBCD_261Ba', 'ECOSUBCD_263Am',\n",
    "                         'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em']\n",
    "\n",
    "numerical_cols = ['DIA', 'HT', 'BasalA', 'LAT', 'LON']\n",
    "categorical_cols = [col for col in independent_variables if col not in numerical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "TLS_X_scaled = scaler.fit_transform(TLS_X_shuffled[numerical_cols])\n",
    "TLS_train_processed = np.hstack((TLS_X_scaled, TLS_X_shuffled[categorical_cols].values))\n",
    "\n",
    "# Predicting the PFTs using the final model\n",
    "predicted_pft_tls = final_model.predict(TLS_train_processed)\n",
    "# Decode predictions for readability\n",
    "predicted_labels_TLS_decoded = le.inverse_transform(predicted_pft_tls)\n",
    "print(\"Length of predicted PFTs:\",len(predicted_labels_TLS_decoded))\n",
    "\n",
    "# Adding the predicted PFTs to the TLS data\n",
    "TLS_X['predicted_PFT'] = predicted_labels_TLS_decoded\n",
    "print(TLS_X['predicted_PFT'].value_counts())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Calculate value counts as percentages\n",
    "percentages = TLS_X['predicted_PFT'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Percentage distribution of predicted PFTs:\")\n",
    "# Round and display\n",
    "percentages = percentages.round(2)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts as percentages\n",
    "percentages = TLS_X.groupby('ECOSUBCD')['predicted_PFT'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Round and display\n",
    "percentages = percentages.round(2)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Field_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the Field data to compare our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field = pd.read_csv(\"./data/03_tree.csv\")\n",
    "df_field_plot = pd.read_csv(\"./data/01_plot_identification.csv\")\n",
    "\n",
    "print(df_field.shape)\n",
    "print(df_field_plot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the scientific names in the field data that are not mapped with the FIA data\n",
    "field_names = set(df_field['tree_sp_scientific_name'].value_counts().index)\n",
    "fia_names = set(df_fia_species['SCI_NAME'].value_counts().index)\n",
    "difference = list(field_names - fia_names)\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping for the differences to maintain uniformity\n",
    "replace_dict = {'Salix spp.': 'Salix sp.', 'Quercus spp.': 'Quercus sp.', 'Cornus nuttallii': 'Cornus nuttalii'} \n",
    "df_fia_species['SCI_NAME'] = df_fia_species['SCI_NAME'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the field data with the plot data and then with the FIA species data\n",
    "df_field = df_field.merge(df_field_plot, on=\"inventory_id\", how=\"left\")\n",
    "df_field = df_field.merge(df_fia_species, left_on=\"tree_sp_scientific_name\", right_on=\"SCI_NAME\", how=\"left\")\n",
    "print(\"Field data shape:\",df_field.shape)\n",
    "df_field.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Deciduous with Deciduous broadleaf as both are the same\n",
    "replace_dict = {'Deciduous': 'Deciduous broadleaf'} \n",
    "df_field['PFT'] = df_field['PFT'].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts as percentages\n",
    "percentages = df_field['PFT'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Round and display\n",
    "percentages = percentages.round(2)\n",
    "print(\"Field data PFT distribution:\")\n",
    "print(percentages)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Predicted TLS data distribution:\")\n",
    "percentages = TLS_X['predicted_PFT'].value_counts(normalize=True) * 100\n",
    "percentages = percentages.round(2)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the overall distribution from our model is pretty close to the actual distribution of the site data, with Evergreen broadleaf distribution being almost the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your mapping\n",
    "site_to_ecosubcd = {\n",
    "    \"Shaver Lake (SHA)\": \"M261Ep\",\n",
    "    \"Sedgwick Reserve (SDR)\": \"261Ba\",\n",
    "    \"Calaveras Big Trees State Park\": \"M261Em\",\n",
    "    \"Pacific Union College (PUC)\": \"263Am\",\n",
    "    \"Independence Lake (IND)\": \"M261Ej\",\n",
    "    \"Winton-Schaads VMP (WIN)\" :\"M261Em\",\n",
    "}\n",
    "\n",
    "# Create new column\n",
    "df_field['ECOSUBCD'] = df_field['site_name_label'].map(site_to_ecosubcd)\n",
    "# Calculate value counts as percentages\n",
    "percentages = df_field.groupby('ECOSUBCD')['PFT'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Round and display\n",
    "percentages = percentages.round(2)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts as percentages\n",
    "percentages = TLS_X.groupby('ECOSUBCD')['predicted_PFT'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Round and display\n",
    "percentages = percentages.round(2)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that even the site wise distributions are pretty close as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the above dataframe for the FIA genus\n",
    "df_fia_genus = df_encoded.copy()\n",
    "df_fia_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the reference species data to get info on GENUS and SPECIES\n",
    "df_ref_species = pd.read_csv('./data/REF_SPECIES.csv')\n",
    "df_ref_species = df_ref_species[['SPCD', 'GENUS', 'SPECIES', 'COMMON_NAME']].copy()\n",
    "print(f\"Shape: {df_ref_species.shape}\")\n",
    "df_ref_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the FIA data with the reference species data\n",
    "df_fia_genus = df_fia_genus.merge(df_ref_species, on=\"SPCD\", how=\"left\")\n",
    "print(f\"Shape: {df_fia_genus.shape}\")\n",
    "df_fia_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of GENUS to keep\n",
    "genus_to_keep = ['Abies', 'Calocedrus', 'Pinus', 'Cornus', 'Populus',\n",
    "                  'Quercus', 'Sequoiadendron', 'Acer', 'Pseudotsuga', 'Arbutus','Salix'\n",
    "                  'Notholicarpos']\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_fia_genus = df_fia_genus[df_fia_genus[\"GENUS\"].isin(genus_to_keep)].copy()\n",
    "df_fia_genus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to remove outliers based on IQR\n",
    "def remove_outliers_by_genus(df, column, group_by):\n",
    "    Q1 = df.groupby(group_by)[column].transform('quantile', 0.25)\n",
    "    Q3 = df.groupby(group_by)[column].transform('quantile', 0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "print(f\"Shape before removing outliers: {df_fia_genus.shape}\")\n",
    "\n",
    "# Remove outliers based on GENUS\n",
    "columns = ['DIA', 'HT', 'BasalA', 'LAT', 'LON']\n",
    "\n",
    "for column in columns:\n",
    "    df_fia_genus = remove_outliers_by_genus(df_fia_genus, column, 'GENUS')\n",
    "\n",
    "# Check the resulting shape\n",
    "print(f\"Shape after removing outliers: {df_fia_genus.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make box plots to see outliers \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENUS', y='DIA', data=df_fia_genus)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of DIA_cm by GENUS')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(x='GENUS', y='HT', data=df_fia_genus)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of HT by GENUS')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENUS', y='BasalA', data=df_fia_genus)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of BasalA by GENUS')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENUS', y='LAT', data=df_fia_genus)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of LAT by GENUS')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='GENUS', y='LON', data=df_fia_genus)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of LON by GENUS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typecasting PFT as cateogorical column\n",
    "df_fia_genus['PFT'] = df_fia_genus['PFT'].astype('category')\n",
    "\n",
    "# selecting important features\n",
    "include_vars = ['DIA', 'HT', 'LAT', 'LON', 'ECOSUBCD_261Ba', 'ECOSUBCD_263Am',\n",
    "                'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em', 'PFT', 'GENUS']\n",
    "df = df_fia_genus[include_vars].dropna().copy()\n",
    "\n",
    "# Create the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the PFT column\n",
    "pft_encoded = ohe.fit_transform(df[[\"PFT\"]])\n",
    "\n",
    "# Convert to DataFrame with appropriate column names\n",
    "pft_encoded_df = pd.DataFrame(\n",
    "    pft_encoded,\n",
    "    columns=[f\"PFT_{cat}\" for cat in ohe.categories_[0]],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Drop the original PFT column and join the encoded columns\n",
    "df = df.drop(columns=[\"PFT\"])\n",
    "df = pd.concat([df, pft_encoded_df], axis=1)\n",
    "\n",
    "df.rename(columns={\"DIA\": \"DIA\"}, inplace=True)\n",
    "X = df.drop(columns=[\"GENUS\"])\n",
    "y = df[\"GENUS\"]\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, stratify=y_encoded , random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Identify numerical and categorical columns again\n",
    "numerical_cols = ['DIA', 'HT', 'LAT', 'LON']\n",
    "categorical_cols = [col for col in X.columns if col not in numerical_cols]\n",
    "\n",
    "# Create column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Concatenate scaled numerical and raw categorical columns\n",
    "X_train_processed = np.hstack((X_train_scaled, X_train[categorical_cols].values))\n",
    "X_test_processed = np.hstack((X_test_scaled, X_test[categorical_cols].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution\n",
    "print(df_fia_genus[\"GENUS\"].value_counts())\n",
    "print(\"Len :\",len(df_fia_genus[\"GENUS\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X, y_encoded are your full datasets with the same number of rows\n",
    "assert len(X) == len(y_encoded), \"X and y_encoded must have the same number of rows.\"\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_encoded)\n",
    "\n",
    "# Check all shapes\n",
    "print(f\"X: {X.shape}, y: {len(y_encoded)}, sample_weights: {len(sample_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the earlier column transformer to preprocess the data\n",
    "X_new = np.hstack((preprocessor.fit_transform(X), X[categorical_cols].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best hyperparameters found earlier\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    class_weight={\n",
    "        0: 0.4252973600232086,\n",
    "        1: 28.745098039215687,\n",
    "        2: 10.471428571428572,\n",
    "        3: 0.4328314142308828,\n",
    "        4: 18.325,\n",
    "        5: 0.36314094624721327,\n",
    "        6: 77.15789473684211,\n",
    "        7: 3.34703196347032,\n",
    "        8: 0.4827132038195588,\n",
    "        9: 61.083333333333336\n",
    "    },\n",
    "    criterion='entropy',\n",
    "    max_depth=100,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=800,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Train and evaluate the best RF model\n",
    "train_and_evaluate(best_rf_model, \"Best Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best hyperparameters from tuning\n",
    "best_et_model = ExtraTreesClassifier(\n",
    "    n_estimators=800,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=None,\n",
    "    max_depth=30,\n",
    "    criterion='entropy',\n",
    "    bootstrap=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function (already defined)\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Train and evaluate the final Extra Trees model\n",
    "train_and_evaluate(best_et_model, \"Best Extra Trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best hyperparameters from tuning\n",
    "best_dt_model = DecisionTreeClassifier(\n",
    "    splitter='best',\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=None,\n",
    "    max_depth=50,\n",
    "    criterion='entropy',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function (already defined)\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Train and evaluate the final Decision Tree model\n",
    "train_and_evaluate(best_dt_model, \"Best Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the meta learner - XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define base models (already trained or initialized)\n",
    "base_models = [\n",
    "    ('rf', best_rf_model),\n",
    "    ('et', best_et_model),\n",
    "    ('dt', best_dt_model)\n",
    "]\n",
    "\n",
    "# Use best config from FLAML to define the meta-learner\n",
    "best_xgb_meta = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.14,\n",
    "    max_depth=3,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.86,\n",
    "    colsample_bytree=0.9,\n",
    "    gamma=0.064,\n",
    "    reg_alpha=0.068,\n",
    "    reg_lambda=1.4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define the final stacking classifier\n",
    "final_stack_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=best_xgb_meta,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "final_stack_model.fit(X_train_processed, y_train_encoded)\n",
    "\n",
    "# Evaluate\n",
    "train_and_evaluate(final_stack_model, \"Stacking with Best-Tuned XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the final stacked model\n",
    "final_stack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data to final stacked model\n",
    "final_stack_model.fit(X_new,y_encoded, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TLS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename the predicted_PFT column to PFT for uniformity\n",
    "TLS_X_genus = TLS_X.copy()\n",
    "TLS_X_genus.columns = ['DIA', 'HT', 'BasalA', 'LAT', 'LON','ECOSUBCD_261Ba','ECOSUBCD_263Am', 'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em', 'ECOSUBCD', 'PFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the PFT column\n",
    "PFT_encoded = pd.get_dummies(TLS_X_genus['PFT'], prefix='PFT')\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "TLS_X_genus = pd.concat([TLS_X_genus, PFT_encoded], axis=1)\n",
    "\n",
    "# Drop the PFT column\n",
    "TLS_X_genus.drop(columns=['PFT'], inplace=True)\n",
    "\n",
    "TLS_X_genus['PFT_Evergreen broadleaf'] = ~TLS_X_genus['PFT_Deciduous broadleaf'] & ~TLS_X_genus['PFT_Evergreen conifer']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "TLS_X_genus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the scaled numerical features with the categorical features\n",
    "TLS_X_cat_n_num_cols = np.hstack((preprocessor.fit_transform(TLS_X_genus), TLS_X_genus[categorical_cols].values))\n",
    "\n",
    "# Predict using the final stacked model\n",
    "pred =  final_stack_model.predict(TLS_X_cat_n_num_cols)\n",
    "predicted_genus = le.inverse_transform(pred)\n",
    "\n",
    "# Adding the predicted GENUS to the TLS data\n",
    "TLS_X_genus['predicted_genus'] = predicted_genus.copy()\n",
    "\n",
    "# checking the distribution\n",
    "TLS_X_genus['predicted_genus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field[['GENUS', 'SPECIES']] = df_field['tree_sp_scientific_name'].str.split(' ', n=1, expand=True)\n",
    "df_field.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fia_genus['GENUS'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of each predicted genus\n",
    "predicted_genus_counts = TLS_X_genus['predicted_genus'].value_counts(normalize=True) * 100\n",
    "# Display the percentage of each predicted genus\n",
    "print(\"Percentage of each predicted genus:\")\n",
    "print(predicted_genus_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field['GENUS'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have predicted Pinus as in training data distribution, Pinus is indeed a predominant Genus, while Abies's distribution is close enought at 19% (Predicted) to 23% (Field data) and Quercus at 20% (Predicted) to 21% (Field data). To predict Genus accurately, more tree specific features could help make model more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPECIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the above dataframe for the FIA genus\n",
    "TLS_species = TLS_X_genus.copy()\n",
    "df_fia_species = df_fia_genus.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution\n",
    "df_fia_species['SPECIES'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'tree_sp_scientific_name' column into two new columns using regex\n",
    "df_field = pd.read_csv(\"data/03_tree.csv\", low_memory=False)\n",
    "df_field[['GENUS_NEW', 'SPECIES_NEW']] = df_field['tree_sp_scientific_name'].str.extract(r'(\\S+)\\s+(.*)')\n",
    "\n",
    "# Display the updated dataframe\n",
    "df_field.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field['SPECIES_NEW'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of species to keep\n",
    "species_to_keep = ['decurrens', 'concolor', 'agrifolia', 'menziesii', 'jeffreyi',\n",
    "                  'kelloggii', 'densiflorus', 'magnificar', 'ponderosa', 'douglasii',\n",
    "                  'lambertiana', 'tremuloides', 'chrysolepis', 'sp.', 'nuttalii', 'lobata', 'giganteum', 'scouleriana', 'glabrum', 'macrophyllum']\n",
    "\n",
    "# Filter the DataFrame\n",
    "df_fia_species = df_fia_species[df_fia_species[\"SPECIES\"].isin(species_to_keep)].copy()\n",
    "df_fia_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the species\n",
    "df_fia_species['SPECIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers based on IQR for each species\n",
    "def remove_outliers_by_species(df, column, group_by):\n",
    "    df_clean = df.copy()\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        Q1 = df_clean.groupby(group_by)[column].transform('quantile', 0.25)\n",
    "        Q3 = df_clean.groupby(group_by)[column].transform('quantile', 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Keep rows within bounds\n",
    "        mask = (df_clean[column] >= lower_bound) & (df_clean[column] <= upper_bound)\n",
    "\n",
    "        prev_shape = df_clean.shape[0]\n",
    "        df_clean = df_clean[mask]\n",
    "        new_shape = df_clean.shape[0]\n",
    "\n",
    "        # If no further rows are removed, break\n",
    "        if new_shape == prev_shape:\n",
    "            print(f\"No more outliers detected after {iteration} iterations.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Iteration {iteration}: Removed {prev_shape - new_shape} rows.\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "columns = ['DIA', 'HT', 'BasalA', 'LAT', 'LON']\n",
    "\n",
    "print(f\"Shape before removing all outliers: {df_fia_species.shape}\")\n",
    "\n",
    "for column in columns:\n",
    "    df_fia_species = remove_outliers_by_species(df_fia_species, column=column, group_by='SPECIES')\n",
    "\n",
    "# Check resulting shape\n",
    "print(f\"Shape after removing all outliers: {df_fia_species.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make box plots to see outliers \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='SPECIES', y='DIA', data=df_fia_species)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of DIA by SPECIES')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(x='SPECIES', y='HT', data=df_fia_species)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of HT by SPECIES')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='SPECIES', y='BasalA', data=df_fia_species)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of BasalA by SPECIES')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='SPECIES', y='LAT', data=df_fia_species)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of LAT by SPECIES')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='SPECIES', y='LON', data=df_fia_species)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of LON by SPECIES')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important features\n",
    "include_vars = ['DIA', 'HT', 'LAT', 'LON', 'ECOSUBCD_261Ba', 'ECOSUBCD_263Am',\n",
    "                'ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em', 'PFT', 'SPECIES', 'GENUS']\n",
    "\n",
    "df = df_fia_species[include_vars].dropna().copy()\n",
    "\n",
    "# Create the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the PFT column\n",
    "pft_encoded = ohe.fit_transform(df[[\"PFT\"]])\n",
    "\n",
    "# Convert to DataFrame with appropriate column names\n",
    "pft_encoded_df = pd.DataFrame(\n",
    "    pft_encoded,\n",
    "    columns=[f\"PFT_{cat}\" for cat in ohe.categories_[0]],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Drop the original PFT column and join the encoded columns\n",
    "df = df.drop(columns=[\"PFT\"])\n",
    "df = pd.concat([df, pft_encoded_df], axis=1)\n",
    "\n",
    "#one-hot encode the GENUS column\n",
    "genus_encoded = ohe.fit_transform(df[[\"GENUS\"]])\n",
    "genus_encoded_df = pd.DataFrame(\n",
    "    genus_encoded,\n",
    "    columns=[f\"GENUS_{cat}\" for cat in ohe.categories_[0]],\n",
    "    index=df.index\n",
    ")\n",
    "# Drop the original GENUS column and join the encoded columns\n",
    "df = df.drop(columns=[\"GENUS\"])\n",
    "df = pd.concat([df, genus_encoded_df], axis=1)\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"SPECIES\"])\n",
    "y = df[\"SPECIES\"]\n",
    "\n",
    "# Encode target variable\n",
    "le_species = LabelEncoder()\n",
    "y_encoded = le_species.fit_transform(y)\n",
    "num_classes = len(le_species.classes_)\n",
    "\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, stratify=y_encoded\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numerical and categorical columns again\n",
    "numerical_cols = ['DIA', 'HT', 'LAT', 'LON']\n",
    "categorical_cols = [col for col in X.columns if col not in numerical_cols]\n",
    "\n",
    "# Create column transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "        # (\"cat\", OneHotEncoder(sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Concatenate scaled numerical and raw categorical columns\n",
    "X_train_processed = np.hstack((X_train_scaled, X_train[categorical_cols].values))\n",
    "X_test_processed = np.hstack((X_test_scaled, X_test[categorical_cols].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_encoded)\n",
    "\n",
    "# Check all shapes\n",
    "print(f\"X: {X.shape}, y: {len(y_encoded)}, sample_weights: {len(sample_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the column transformer to the dataset\n",
    "X_new = np.hstack((preprocessor.fit_transform(X), X[categorical_cols].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the base learner - Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best tuned model configuration\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    class_weight={\n",
    "        0: 3.6483766233766235,\n",
    "        1: 0.6677561207511291,\n",
    "        2: 0.25883355599576174,\n",
    "        3: 0.4001210653753027,\n",
    "        4: 9.909171075837742,\n",
    "        5: 33.44345238095238,\n",
    "        6: 1.182095518619819,\n",
    "        7: 1.141739483844747,\n",
    "        8: 1.182095518619819,\n",
    "        9: 133.77380952380952,\n",
    "        10: 17.448757763975156,\n",
    "        11: 1.4462033462033461,\n",
    "        12: 0.41869736940159474,\n",
    "        13: 44.59126984126984\n",
    "    },\n",
    "    max_depth=100,\n",
    "    max_features='log2',\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Run final training + evaluation\n",
    "train_and_evaluate(best_rf_model, \"Best Random Forest (Final)\")\n",
    "\n",
    "#test the random forest model on the test set\n",
    "predictions = best_rf_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "report = classification_report(y_test_encoded, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the base learner - Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best tuned Extra Trees model config\n",
    "best_et_model = ExtraTreesClassifier(\n",
    "    n_estimators=600,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    max_depth=None,\n",
    "    criterion='gini',\n",
    "    bootstrap=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Run final training + evaluation\n",
    "train_and_evaluate(best_et_model, \"Best Extra Trees (Final)\")\n",
    "\n",
    "predictions = best_et_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "report = classification_report(y_test_encoded, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the base learner - Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Best tuned Decision Tree model config\n",
    "best_dt_model = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=50,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluation function (assumed already defined)\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Train + evaluate the final Decision Tree model\n",
    "train_and_evaluate(best_dt_model, \"Best Decision Tree (Final)\")\n",
    "\n",
    "predictions = best_dt_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "report = classification_report(y_test_encoded, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the meta-learner - Stacking with XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# Reuse base models\n",
    "base_models = [\n",
    "    ('rf', best_rf_model),\n",
    "    ('et', best_et_model),\n",
    "    ('dt', best_dt_model)\n",
    "]\n",
    "\n",
    "# Meta-learner with best-found XGB config\n",
    "best_xgb_meta = XGBClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    min_child_weight=2,\n",
    "    subsample=1.0,               \n",
    "    colsample_bytree=0.9,\n",
    "    gamma=0,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Final stacking model\n",
    "best_stack_xgb_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=best_xgb_meta,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def train_and_evaluate(model, name):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    predictions = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "    report = classification_report(y_test_encoded, predictions)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Training Time: {training_time:.2f} seconds\")\n",
    "    print(report)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Train + evaluate final stacking model\n",
    "train_and_evaluate(best_stack_xgb_model, \"Final Stacking with XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data to final stacked model\n",
    "best_stack_xgb_model.fit(X_new,y_encoded, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "predictions = best_stack_xgb_model.predict(X_test_processed)\n",
    "accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "predictions = le_species.inverse_transform(predictions)\n",
    "y_test_labels = le_species.inverse_transform(y_test_encoded)  # Transform y_test_encoded to original labels\n",
    "report = classification_report(y_test_labels, predictions)  # Use transformed labels\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_field['SPECIES_NEW'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with TLS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the PFT column\n",
    "genus_encoded = pd.get_dummies(TLS_species['predicted_genus'], prefix='GENUS')\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "TLS_species = pd.concat([TLS_species, genus_encoded], axis=1)\n",
    "\n",
    "# Drop the original PFT column\n",
    "TLS_species.drop(columns=['predicted_genus'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "TLS_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding these two columns as they are not predicted by the model and settinhg them to false to match the training dimensions\n",
    "# This still means that the genus for current model is not Arbutus, Lithocarpus, Populus\n",
    "TLS_species[['GENUS_Arbutus', 'GENUS_Sequoiadendron', 'GENUS_Populus']] = False, False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features for model prediction\n",
    "TLS_species_features = TLS_species[['DIA', 'HT', 'LAT', 'LON', 'ECOSUBCD_261Ba', 'ECOSUBCD_263Am','ECOSUBCD_M261Ej', 'ECOSUBCD_M261Em', 'PFT_Deciduous broadleaf',\n",
    "       'PFT_Evergreen broadleaf', 'PFT_Evergreen conifer', 'GENUS_Abies',\n",
    "       'GENUS_Acer', 'GENUS_Calocedrus', 'GENUS_Pinus', 'GENUS_Pseudotsuga',\n",
    "       'GENUS_Quercus', 'GENUS_Sequoiadendron', 'GENUS_Arbutus',\n",
    "       'GENUS_Populus']]\n",
    "len(TLS_species_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in TLS_species_features.columns if col not in numerical_cols]\n",
    "TLS_species_features_array = np.hstack((preprocessor.fit_transform(TLS_species), TLS_species[categorical_cols].values))\n",
    "#predictions\n",
    "pred =  best_stack_xgb_model.predict(TLS_species_features_array)\n",
    "predicted_species = le_species.inverse_transform(pred)\n",
    "TLS_species['predicted_species'] = predicted_species.copy()\n",
    "TLS_species['predicted_species'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data distribution:\")\n",
    "print(df_fia_species['SPECIES'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 30)\n",
    "\n",
    "# find percentage of each predicted genus\n",
    "predicted_species_counts = TLS_species['predicted_species'].value_counts(normalize=True) * 100\n",
    "# Display the percentage of each predicted genus\n",
    "print(\"Percentage of each predicted species:\")\n",
    "print(predicted_species_counts)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Field data distribution\")\n",
    "print(df_field[\"SPECIES_NEW\"].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have decurrens predicted as 25% (Predicted) to 22% (Field data) and concolor coming in close at 19% (Predicted) to 20% (Field)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote-test",
   "language": "python",
   "name": "remote-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
